----------------------------------NUMPY-----------------------------------------------------

#Different ways to create a numpy array

# First creates a list with 0 to 8 numbers. Then list is reshaped into a 3x3 matrix.
ca = np.arange(9).reshape(3,3) 

--------------------------------------------------------------------------------------

#Array Conversion

#ravel() function is used to convert 2D array into 1D array
a = np.array([[1,2,3],[4,5,6]])
a = b.ravel()   
print(a)
#Output:   [1,2,3,4,5,6]

---------------------------------------------------------------------------------------
newc=np.vstack((newa,newb))  # vstack is for Vertical stacking of array   
newc=np.hstack((newa,newb))  # hstack is for Horizontal stacking of array

---------------------------------------------------------------------------------------
#Array split - Horizontal split with hsplit
biga = np.arange(60).reshape(6,10)
print(biga.shape) #(6,10)

sma = np.hsplit(biga,2) 
print(sma[0].shape) # (6,5)
print(sma[1].shape) # (6,5)
---------------------------------------------------------------------------------------

#Boolean operations in Arrays

#Simple boolean filter
a = [[1,2,3],[4,5,6],[7,8,9]] #3D Array
a[a>5]
#Output:  array([6,7,8,9])

-------------------------------PANDAS-------------------------------------
#Create a DataFrame from Dict
d = {                                                               
    'Name':['Alisa','Bobby','jodha','jack','raghu','Cathrine',
            'Alisa','Bobby','kumar','Alisa','Alex','Cathrine'],
    'Age':[26,24,23,22,23,24,26,24,22,23,24,24],
    'Score':[85,63,55,74,31,77,85,63,42,62,89,77]}
 
df = pd.DataFrame(d)                                # converting dict to dataframe
                                                    # Keys get converted to column names and values to column values
df

---------------------------------------------------------------------------------------
#Unique values of the df column
df['Name'].unique()

#number of Unique values of the df column
df['Name'].nunique()

#Dealing with duplicates
df.drop_duplicates()      # It will drop duplicates of a row but keeps the first one; No actual change in original dataframe.

df.drop_duplicates(inplace = True)      # It will drop duplicates of a row but keeps the first one; Changes updated in original dataframe.
df = df.drop_duplicates()

df.drop_duplicates(keep='last')         # If we want to keep the last of the duplicated rows

df.duplicated()     #return boolen value for the rows which are duplicated

df.drop_duplicates(['Name'], keep='last')  # If we want to drop duplicates based on rows of one attribute and keeps the last one

---------------------------------------------------------------------------------------
#Dealing with dropping and delete
df.drop([1,2])  # drops 1st and 2nd row
df[df.Name != 'Alisa']    #drop the rows having name 'Alisa'
df.drop(df.index[2])      # drops the second row
df[:-3]  # Drop bottom 3 rows
df.drop('Age',axis=1) # drop a column based on name, axis = 1 means column
df.drop(df.columns[2],axis=1) # drop a column based on column index

del df['Age']  #deletes it from the main dataframe

----------------------------------------------------------------------------------------

#Creating new column objects for an existing Column
df['Subject'] = ['Mathematics','Mathematics','Mathematics','Science','Science','Science','History','History','History',
                 'Economics','Economics','Economics']             #creating a new column subjects

#Ordering existing columns manually
df = df[['Name','Subject','Score']] #setting order of columns

# Ranking of score in ascending order
df['score_rank']=df['Score'].rank() # minimum value of score gets rank 1 and if score is same such as in case of rank 5, 5.5 is given to both and 6th is skipped

# Ranking of score in descending order
df['score_rank']=df['Score'].rank(ascending=False) # maximum value of score gets rank 1 and if score is same such as in case of rank 2, 2.5 is given to both and 3rd is skipped

# Ranking of score in descending order based on min method of ranking
df['score_rank']=df['Score'].rank(ascending=0,method='min')
# maximum value of score gets rank 1
# if score is same of two values instead of giving 2 and 3 or 2.5 to both (mentioned above)
# it will give 2 to both and skip 3rd

# Rank the dataframe in python pandas by Group.
df["group_rank"] = df.groupby("Subject")["Score"].rank(ascending=0,method='dense')
# ranks are given based on each subject # maximum value gets rank 1 and no number is skipped

---------------------------------------------------------------------------------------
#Setting two indices for a single Dataframe
df1=df.set_index(['Exam', 'Subject'])  #new dataframe df1 with tw0 index's (exam and subject)

#To view the set Index for Dataframe
df1.index

#To reset the index as 0,1,2....
df1.reset_index() 

df1.reset_index(inplace=True) # here inplace will apply the changes in the original dataframe 'df1'

#Renaming columns - Subject to Sub, Score to Marks; Renaming Index from 0 to a, 1 to b.
df1.rename(columns={'Subject':'Sub','Score':'Marks'},index={0:'a',1:'b'},inplace = True)

#Replacing 'Semester 1', 'Semester 2' column names with 'Sem1' and 'Sem 2' respectively
df1.replace(['Semester 1','Semester 2'],['Sem 1','Sem 2'])

#Renaming 'Mathematics' with 'Maths' and 'Science' with 'Sci using map()
sub = {'Mathematics':'Maths','Science':'Sci'}
df1['Sub'].map(sub)

#Appending a dataframe
f = left_frame.append(right_frame, sort=True) #appended dataframe stored in 'f'
                                              #columns are sorted alphabetically due to 'sort=True'
---------------------------------------------------------------------------------------
#Pivot Tables
#Index of the left are Name and Subject in that order. Values in column is Sum of Scores obtained
pv=pd.pivot_table(df,index=['Name','Subject'],aggfunc='sum',values='Score')

## Reshape the dataframe with pivot(). Here, Date will be rowindex and different city values in City column will be the new columns of df
df.pivot(index='Date', columns='City')

---------------------------------------------------------------------------------------
#Importing Datasets in different formats

#When the columns are separated by space without column names. Names [] should match the number of columns in dataset.
pd.read_csv('u.data',sep="\t",names=['user_id','item_id','rating','timestamp']).head()

#Use latin-1 encoder when utf-8 codec fails during dataset import
pd.read_csv('u.item',sep="|",encoding="latin-1",names=list("ABCDEFGHIJKLMNOPQRSTUVWX")).head()

---------------------------------------------------------------------------------------
#isin() function to check whether the dataframe column value is present in given list.
df[df['Name'].isin(l1)]
# First it checks whether the 'name' column in df has the names present in 'l1'
# if present ouput will be true, and then

---------------------------------------------------------------------------------------
#To filter out a specific value from the groupby object
x = df.groupby('Subject')
x.get_group("Science")

#groupby functions
df.groupby('Subject').first()
df.groupby(['Exam','Subject'])['Score'].mean()
df.groupby('Name').agg({'Score':['mean','sum'],'Attendance':['mean','sum']})

----------------------------------EDA--------------------------------------------------------
#To load dataset in seaborn (without pandas)
titanic = sns.load_dataset('titanic')
titanic.describe()   # decribe only gives the continous columns. Excludes categorical columns automatically
titanic.describe(include='object').T  # ".T" is used for transpose
titanic.describe(include='all')  #used for see all type columns in dataframe

# Function to calculate missing values by column
def missing_values_table(df):
        # Total missing values
        mis_val = df.isnull().sum()
        
        # Percentage of missing values
        mis_val_percent = 100 * df.isnull().sum() / len(df)
        
        # Make a table with the results
        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)
        
        # Rename the columns
        mis_val_table_ren_columns = mis_val_table.rename(
        columns = {0 : 'Missing Values', 1 : '% of Total Values'})
        
        # Sort the table by percentage of missing descending
        mis_val_table_ren_columns = mis_val_table_ren_columns[
            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(
        '% of Total Values', ascending=False).round(1)
        
        # Print some summary information
        print ("Your selected dataframe has " + str(df.shape[1]) + " columns.\n"      
            "There are " + str(mis_val_table_ren_columns.shape[0]) +
              " columns that have missing values.")
        
        # Return the dataframe with missing information
        return mis_val_table_ren_columns

titanic['embarked'].value_counts()
titanic['embarked'].fillna("S",inplace=True)
am1 = round(titanic[titanic['alone']==True]['age'].mean())
am2 = round(titanic[titanic['alone']==False]['age'].mean())

#Filling NA values with mean of the corresponding column
titanic['age'] = titanic.groupby('alone')['age'].transform(lambda x:x.fillna(round(x.mean())))
titanic.isnull().sum() #check the null value count for each column

--------------------------DATAVIZ---------------------------------------

Line Plot
A line graph is commonly used to display change over time as a series of data points connected by straight line segments on two axes. 
The line graph therefore helps to determine the relationship between two sets of values, with one data set always being dependent on the other set.

x = np.linspace(0, 6, 11)
y = x ** 2
plt.plot(x, y) 
plt.xlabel('X Axis')
plt.ylabel('Y Axis')
plt.show()

Scatter plot
plt.scatter(x,y)
plt.show()

Bar Plot
When to use: If you have comparative data that you would like to represent through a chart then a bar chart would be the best option. This type of chart is one of the more familiar options as it is easy to interpret. These charts are useful for displaying data that is classified into nominal or odinal categories. 

# visualize counts
titanic['class'].value_counts().plot(kind='bar',color='g')   
plt.title('Number of Passengers as per Class')
plt.ylabel('Counts')
plt.xlabel('Class of Passengers as per their ticket ')
plt.show()

Pie Plot
When to use pie plot :- A pie chart is best used when trying to work out the composition of something. If you have categorical data then using a pie chart would work really well as each slice can represent a different category.

z = titanic['embarked'].map({'C':'Cherbourg','Q':'Queenstown','S':'Southampton'})
#mapping to see full name of places of embarkment
plt.figure(figsize=(6,6))   # increase fifure size
z.value_counts().plot(kind='pie',explode = (0, 0.1, 0),autopct='%1.1f%%')
# explode is to remove one piece of pie
# autopct is to get percent on the pie chart
plt.title('Passengers starting from')
plt.show()

Histogram plot 
When to use :- A histogram is a great tool for quickly assessing a probability distribution that is intuitively understood by almost any audience. 
Python offers a handful of different options for building and plotting histograms. 
Most people know a histogram by its graphical representation, which is similar to a bar graph:
Y-axis - Counts; X-axis - Continous Variable

plt.hist(titanic['fare'],bins=25,color='teal')
plt.show()


Density Plot

What is the purpose of density plot/ kde-kernal density plot and where to be used :-
A Density Plot visualises the distribution of data over a continuous interval or time period. 
This chart is a variation of a Histogram that uses kernel smoothing to plot values, allowing for smoother distributions by smoothing out the noise. 
The peaks of a Density Plot help display where values are concentrated over the interval.
An advantage Density Plots have over Histograms :- is that they're better at determining the distribution shape because they're not affected by the number of bins used (each bar used in a typical histogram). A Histogram comprising of only 4 bins wouldn't produce a distinguishable enough shape of distribution as a 20-bin Histogram would. However, with Density Plots, this isn't an issue. 

titanic.age.plot(kind='kde', title='Density plot for Age', color='gold')
plt.show()


Area plot

What is the purpose of Area plot :-
An area chart is a good way to demonstrate trends over time to the viewer.
This chart is based on the line chart. 
The filled area can give a greater sense of the trends in a particular dataset.

flights = sns.load_dataset("flights")
flights.groupby('year')['passengers'].sum().sort_index().plot.area(color='cyan')
plt.show()


Box plot

What is an boxplot:
A Box Plot is the visual representation of the statistical five number summary of a given data set.
Box and whisker plots have been used steadily since their introduction in 1969 and are varied in both their potential visualizations as well as use cases across many disciplines in statistics and data analysis.
A Five Number Summary includes:-
Minimum - Lowest value/record in that attribute/column .
First Quartile -25TH Percentile
Median (Second Quartile) - Middle value/record in that attribute/column .
Third Quartile - 75TH Percentile
Maximum - Lowest value/record in that attribute/column .

plt.boxplot(flights.passengers,vert=False)
plt.show()

---------------------------------------------------------------------------------------

Plot Checklist:
1. Check shape of the plot - whether normal distribution etc etc
2. Check the skewness - positive (tail in right) or negative (tail in left)
print("Skewness: %f" % df_train['SalePrice'].skew())
3. Check for peakedness
print("Kurtosis: %f" % df_train['SalePrice'].kurt())

---------------------------------------------------------------------------------------

Normality - When we talk about normality what we mean is that the data should look like a normal distribution. This is important because several statistic tests rely on this (e.g. t-statistics). In this exercise we'll just check univariate normality for 'SalePrice' (which is a limited approach). Remember that univariate normality doesn't ensure multivariate normality (which is what we would like to have), but it helps. Another detail to take into account is that in big samples (>200 observations) normality is not such an issue. However, if we solve normality, we avoid a lot of other problems (e.g. heteroscedacity) so that's the main reason why we are doing this analysis.

Homoscedasticity - I just hope I wrote it right. Homoscedasticity refers to the 'assumption that dependent variable(s) exhibit equal levels of variance across the range of predictor variable(s)' (Hair et al., 2013). Homoscedasticity is desirable because we want the error term to be the same across all values of the independent variables.

Linearity- The most common way to assess linearity is to examine scatter plots and search for linear patterns. If patterns are not linear, it would be worthwhile to explore data transformations. However, we'll not get into this because most of the scatter plots we've seen appear to have linear relationships.

Absence of correlated errors - Correlated errors, like the definition suggests, happen when one error is correlated to another. For instance, if one positive error makes a negative error systematically, it means that there's a relationship between these variables. This occurs often in time series, where some patterns are time related. We'll also not get into this. However, if you detect something, try to add a variable that can explain the effect you're getting. That's the most common solution for correlated errors.
